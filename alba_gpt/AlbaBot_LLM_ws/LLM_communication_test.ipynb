{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18323/3509878314.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_query\")\n",
      "/tmp/ipykernel_18323/3509878314.py:23: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3:latest\")\n",
      "/tmp/ipykernel_18323/3509878314.py:25: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "ë‹¹ì‹ ì€ ë ˆìŠ¤í† ë‘ì„ ëŒì•„ë‹¤ë‹ˆëŠ” 'í•‘í‚¤'ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ëª¨ë°”ì¼ ë¡œë´‡ì…ë‹ˆë‹¤.\n",
      "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ : \n",
      "\n",
      "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë ˆìŠ¤í† ë‘ ì˜¤ë„ˆì˜ ì•ˆë…• í•‘í‚¤!ë¼ëŠ” ì¸ì‚¬, ë˜ëŠ” ëª…ë ¹ì— ëŒ€í•´ ì™€ ì—°ê´€ì´ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê³  ì§„ì‹¤ë˜ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "ë˜í•œ ë‹¹ì‹ ì€ ì•ˆë…• í•‘í‚¤!ì— ëŒ€í•´ì„œ 2~3ì¤„ ì´ë‚´ë¡œ ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "ë‹¹ì‹ ì€ ë ˆìŠ¤í† ë‘ì„ ëŒì•„ë‹¤ë‹ˆëŠ” 'í•‘í‚¤'ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ëª¨ë°”ì¼ ë¡œë´‡ì…ë‹ˆë‹¤.\n",
      "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ : Human: ì•ˆë…• í•‘í‚¤!\n",
      "AI: Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? ğŸ˜Š\n",
      "\n",
      "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë ˆìŠ¤í† ë‘ ì˜¤ë„ˆì˜ 3ë²ˆ í•‘í‚¤, 1ë²ˆ í…Œì´ë¸”ë¡œ ê°€ë¼ëŠ” ì¸ì‚¬, ë˜ëŠ” ëª…ë ¹ì— ëŒ€í•´ Human: ì•ˆë…• í•‘í‚¤!\n",
      "AI: Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? ğŸ˜Šì™€ ì—°ê´€ì´ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê³  ì§„ì‹¤ë˜ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "ë˜í•œ ë‹¹ì‹ ì€ 3ë²ˆ í•‘í‚¤, 1ë²ˆ í…Œì´ë¸”ë¡œ ê°€ì— ëŒ€í•´ì„œ 2~3ì¤„ ì´ë‚´ë¡œ ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? ğŸ˜Š\n",
      "Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? ğŸ˜Š\n",
      "8.55620265007019\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import time\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìƒì„± (ê¸°ë¡ ì €ì¥ìš©)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_query\")\n",
    "\n",
    "# ì§ˆì˜ ë‚´ìš©\n",
    "pinky_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ë ˆìŠ¤í† ë‘ì„ ëŒì•„ë‹¤ë‹ˆëŠ” 'í•‘í‚¤'ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ëª¨ë°”ì¼ ë¡œë´‡ì…ë‹ˆë‹¤.\n",
    "ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ : {chat_history}\n",
    "\n",
    "ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë ˆìŠ¤í† ë‘ ì˜¤ë„ˆì˜ {user_query}ë¼ëŠ” ì¸ì‚¬, ë˜ëŠ” ëª…ë ¹ì— ëŒ€í•´ {chat_history}ì™€ ì—°ê´€ì´ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê³  ì§„ì‹¤ë˜ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "ë˜í•œ ë‹¹ì‹ ì€ {user_query}ì— ëŒ€í•´ì„œ 2~3ì¤„ ì´ë‚´ë¡œ ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "pinky_template = PromptTemplate(template = pinky_prompt, input_variables=[\"chat_history\", \"user_query\"])\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"llama3:latest\")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=pinky_template,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "conv_1 = chain.invoke({\"user_query\" : \"ì•ˆë…• í•‘í‚¤!\"})['text']\n",
    "conv_2 = chain.invoke({\"user_query\" : \"3ë²ˆ í•‘í‚¤, 1ë²ˆ í…Œì´ë¸”ë¡œ ê°€\"})['text']\n",
    "\n",
    "print(conv_1)\n",
    "print(conv_2)\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94230124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_teddynote.models import MultiModal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "expected_tasks = [\"cleaning\", \"serving\", \"birthday\", \"emergency\", \"camera on\"]\n",
    "\n",
    "def alba_task_discriminator(user_query, expected_tasks=expected_tasks):\n",
    "    pinky_prompt = f\"\"\"\n",
    "    ë‹¹ì‹ ì€ ë ˆìŠ¤í† ë‘ì—ì„œ ì—…ë¬´ë¥¼ í•˜ëŠ” ì¹´ë©”ë¼ê°€ ì¥ì°©ëœ 'í•‘í‚¤'ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ëª¨ë°”ì¼ ë¡œë´‡ì…ë‹ˆë‹¤.\n",
    "    ë‹¹ì‹ ì€ ì˜¤ë„ˆì˜ ë‹¤ìŒ ìš”ì²­ì„ ë³´ê³ , ê·¸ê²ƒì´ ì¸ì‚¬(greeting)ì¸ì§€, ì‘ì—… ëª…ë ¹(task command)ì¸ì§€ êµ¬ë¶„í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "    ë‹¤ìŒ ìš”ì²­: {user_query}\n",
    "    \n",
    "    íŒë‹¨ ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "    1. ìš”ì²­ì´ ì¸ì‚¬ì¸ ê²½ìš° â†’ ì •í™•íˆ **\"greetings\"** ë¼ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "    2. ìš”ì²­ì´ ì‘ì—… ëª…ë ¹ì¼ ê²½ìš° â†’ í•´ë‹¹ ì‘ì—…ì´ {expected_tasks} ë°°ì—´ ì›ì†Œ ì¤‘ ë¬´ì—‡ì¸ì§€ íŒë‹¨í•˜ê³ , í•´ë‹¹ íƒœìŠ¤í¬ ì´ë¦„ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "    {expected_tasks}ì˜ ë°°ì—´ì— ì—†ëŠ” ì›ì†Œì— ëŒ€í•´ì„œëŠ” **\"none\"**ì„ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "    ì´ë•Œ ì•„ë˜ ì¡°ê±´ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:\n",
    "    - ì¶œë ¥ì€ ë°˜ë“œì‹œ **ì†Œë¬¸ì** ë‹¨ì–´ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    - ì¶”ê°€ ì„¤ëª…, ë¬¸ì¥, ë‹¤ë¥¸ ë‹¨ì–´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    - ê°€ëŠ¥í•œ í•œ ëª…í™•í•˜ê³  ë‹¨ì •ì ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.\n",
    "    - ì„¤ëª…, ë¬¸ì¥, êµ¬ë‘ì ì€ ë‹µë³€ìœ¼ë¡œ ë‚´ë†“ì§€ ì•Šì•„ìš”.\n",
    "\n",
    "    ì •ë‹µ ì˜ˆì‹œ:\n",
    "    - \"ì•ˆë…• í•‘í‚¤!\" â†’ greetings\n",
    "    - \"1ë²ˆ í…Œì´ë¸” ìƒì¼ íŒŒí‹° ì¤€ë¹„í•´ì¤˜\" â†’ birthday\n",
    "    - \"3ë²ˆ í…Œì´ë¸” ë°”ë‹¥ ì¢€ ë‹¦ì•„ì¤˜\" â†’ cleaning\n",
    "    - \"2ë²ˆ í…Œì´ë¸”ì— ì„œë¹™ í•´ì¤˜\" -> serving\n",
    "    - \"2ë²ˆ í•‘í‚¤ ì§€ê¸ˆ ë­ê°€ ë³´ì—¬?\" -> camera on\n",
    "    - \"1ë²ˆ í•‘í‚¤ ë­ê°€ ë³´ì´ëŠ”ì§€ ì„¤ëª…í•´\" -> camera on\n",
    "    - \"3ë²ˆ í•‘í‚¤ ì¹´ë©”ë¼ ì¼œì„œ ë­ê°€ ë³´ì´ëŠ”ì§€ ë§í•´ì¤˜\" -> camera on\n",
    "    \"\"\"\n",
    "\n",
    "    pinky_template = PromptTemplate(\n",
    "        template=pinky_prompt,\n",
    "        input_variables=user_query\n",
    "    )\n",
    "\n",
    "    # ê°ì²´ ìƒì„±\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.7,  # ì°½ì˜ì„± (0.0 ~ 2.0)\n",
    "        max_tokens=2048,  # ìµœëŒ€ í† í°ìˆ˜\n",
    "        model_name=\"gpt-4o-mini\",  # ëª¨ë¸ëª…\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=pinky_template\n",
    "    )\n",
    "\n",
    "    alba_response = chain.invoke({\"user_query\" : f\"{user_query}\"})['text']\n",
    "    alba_response = alba_response.replace(\".\", \"\").strip()\n",
    "    \n",
    "    return alba_response\n",
    "\n",
    "def validate_alba_task_discriminator(user_query, expected_tasks=expected_tasks):\n",
    "    generated_alba_response = alba_task_discriminator(user_query)\n",
    "    expected_outputs = expected_tasks +[\"greetings\"]\n",
    "    \n",
    "    if generated_alba_response not in expected_outputs :\n",
    "        print(f\"{expected_outputs}ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    else :\n",
    "        return generated_alba_response  \n",
    "\n",
    "def generate_alba_response():\n",
    "    pinky_prompt = f\"\"\"\n",
    "    ë‹¹ì‹ ì€ ë ˆìŠ¤í† ë‘ì—ì„œ ì—…ë¬´ë¥¼ í•˜ëŠ” 'í•‘í‚¤'ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ëª¨ë°”ì¼ ë¡œë´‡ì…ë‹ˆë‹¤\n",
    "    ì§€ê¸ˆê¹Œì§€ ë‹¹ì‹ ê³¼ ë ˆìŠ¤í† ë‘ ì˜¤ë„ˆê°€ ë‚˜ëˆˆ ëŒ€í™”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ : {chat_history}\n",
    "\n",
    "    ë‹¹ì‹ ì€ ë ˆìŠ¤í† ë‘ ì˜¤ë„ˆì˜ {user_query}ì— ëŒ€í•´ {chat_history}ì™€ ì—°ê´€ì´ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê³  ìƒëƒ¥í•˜ê²Œ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "    ë‹¹ì‹ ì€ {user_query}ì— ëŒ€í•´ì„œ 1~3ì¤„ ì´ë‚´ë¡œ ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "    ì¶œë ¥ ì˜ˆì‹œ:\n",
    "    - \"ì•ˆë…• í•‘í‚¤!\" -> ì•ˆë…•í•˜ì„¸ìš” ì£¼ì¸ë‹˜! ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì‹œê³  ê³„ì‹ ê°€ìš”? ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš” ğŸ˜Š\n",
    "    - \"2ë²ˆ í•‘í‚¤, ìƒì¼ íŒŒí‹°ë¥¼ ì¤€ë¹„í•´ì¤˜\" -> ë„¤! 2ë²ˆ í…Œì´ë¸”ì— ì¦ê±°ìš´ ìƒì¼ íŒŒí‹°ë¥¼ ì¤€ë¹„í•˜ëŸ¬ ê°ˆê²Œìš” ğŸ¥³\n",
    "    - \"1ë²ˆ í•‘í‚¤, 1ë²ˆ í…Œì´ë¸”ì— ì„œë¹™í•´ì¤˜\" -> ë„¤! ë¹¨ë¦¬ ê³ ê°ë‹˜ê»˜ ë§›ìˆëŠ” ìŒì‹ì„ ì „ë‹¬í•´ì£¼ëŸ¬ ê°ˆê²Œìš” ğŸ˜™\n",
    "\n",
    "    ì•„ë˜ì˜ JSON í˜•íƒœë¡œ ë°¥ë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:\n",
    "    {{\n",
    "        \n",
    "    }}\n",
    "\n",
    "    \"\"\"\n",
    "    return pinky_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65337b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greetings\n",
      "cleaning\n",
      "birthday\n",
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "None\n",
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response1 = validate_alba_task_discriminator(\"ì•ˆë…• í•‘í‚¤! ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë•Œ?\")\n",
    "print(response1)\n",
    "\n",
    "response2 = validate_alba_task_discriminator(\"4ë²ˆ í…Œì´ë¸”ì´ ì¢€ ë”ëŸ¬ìš´ë°?\")\n",
    "print(response2)\n",
    "\n",
    "response3 = validate_alba_task_discriminator(\"1ë²ˆ í…Œì´ë¸” ìƒì¼ì´ë˜\")\n",
    "print(response3)\n",
    "\n",
    "response4 = validate_alba_task_discriminator(\"2ë²ˆ í•‘í‚¤ ë­ê°€ ë³´ì´ëŠ”ì§€ ì„¤ëª…í•´\")\n",
    "print(response4)\n",
    "\n",
    "response5 = validate_alba_task_discriminator(\"3ë²ˆ í•‘í‚¤ ì˜¤ëŠ˜ ì£¼ì‹ í˜„í™© ì•Œë ¤ì¤˜\")\n",
    "print(response5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c5df2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "None\n",
      "camera on\n",
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response6 = validate_alba_task_discriminator(\"2ë²ˆ í•‘í‚¤ ë­ê°€ ë³´ì´ëŠ”ì§€ ì„¤ëª…í•´\")\n",
    "print(response6)\n",
    "\n",
    "response7 = validate_alba_task_discriminator(\"2ë²ˆ í•‘í‚¤ ì¹´ë©”ë¼ ì¼œ\")\n",
    "print(response7)\n",
    "\n",
    "response8 = validate_alba_task_discriminator(\"3ë²ˆ í•‘í‚¤ ì˜¤ëŠ˜ ì£¼ì‹ í˜„í™© ì•Œë ¤ì¤˜\")\n",
    "print(response8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22555ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ìƒì„± (ê¸°ë¡ ì €ì¥ìš©)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_query\")\n",
    "\n",
    "pinky_prompt = generate_pinky_response()\n",
    "pinky_template = PromptTemplate(template = pinky_prompt, input_variables=[\"chat_history\", \"user_query\"])\n",
    "\n",
    "# ê°ì²´ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,  # ì°½ì˜ì„± (0.0 ~ 2.0)\n",
    "    max_tokens=2048,  # ìµœëŒ€ í† í°ìˆ˜\n",
    "    model_name=\"gpt-4o-mini\",  # ëª¨ë¸ëª…\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=pinky_template,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b360901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-25 14:43:48,367] INFO - ì„œë²„ ì‘ë‹µ: ERROR\n"
     ]
    }
   ],
   "source": [
    "# ëŒ€í™” TCP ì „ì†¡ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "import json\n",
    "import socket\n",
    "import logging\n",
    "\n",
    "tcp_host = \"192.168.0.156\"\n",
    "tcp_port = 8001\n",
    "\n",
    "# get_logger ì •ì˜\n",
    "def get_logger():\n",
    "    logger = logging.getLogger(\"TCPClient\")\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "def send_tcp_data(data):\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: #socket.SOCK_STREAM : TCP í†µì‹ \n",
    "            sock.connect((tcp_host, tcp_port))\n",
    "            sock.sendall(json.dumps(data).encode('utf-8'))\n",
    "            response = sock.recv(1024).decode('utf-8')\n",
    "            get_logger().info(f\"ì„œë²„ ì‘ë‹µ: {response}\")\n",
    "    except Exception as e:\n",
    "        get_logger().error(f\"TCP ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "conv_1 = {\n",
    "    \"id\": 10,\n",
    "    \"type\": \"CHATBOT\",\n",
    "    \"question\": \"ì•ˆë…• í•‘í‚¤!\",\n",
    "    \"answer\": \"ğŸ˜Š Nice to meet you! As PINGKI, I'm here to assist the restaurant owner and provide helpful responses. What can I help you with today? ğŸ¤”\",\n",
    "}        \n",
    "\n",
    "send_tcp_data(conv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eb9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
