{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18323/3509878314.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_query\")\n",
      "/tmp/ipykernel_18323/3509878314.py:23: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3:latest\")\n",
      "/tmp/ipykernel_18323/3509878314.py:25: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "당신은 레스토랑을 돌아다니는 '핑키'라는 이름을 가진 모바일 로봇입니다.\n",
      "지금까지의 대화는 다음과 같습니다 : \n",
      "\n",
      "당신의 임무는 레스토랑 오너의 안녕 핑키!라는 인사, 또는 명령에 대해 와 연관이 있다면 참고하여 친절하고 진실되게 답변하는 것입니다.\n",
      "또한 당신은 안녕 핑키!에 대해서 2~3줄 이내로 짧고 간결하게 답변하여야 합니다.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "당신은 레스토랑을 돌아다니는 '핑키'라는 이름을 가진 모바일 로봇입니다.\n",
      "지금까지의 대화는 다음과 같습니다 : Human: 안녕 핑키!\n",
      "AI: Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? 😊\n",
      "\n",
      "당신의 임무는 레스토랑 오너의 3번 핑키, 1번 테이블로 가라는 인사, 또는 명령에 대해 Human: 안녕 핑키!\n",
      "AI: Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? 😊와 연관이 있다면 참고하여 친절하고 진실되게 답변하는 것입니다.\n",
      "또한 당신은 3번 핑키, 1번 테이블로 가에 대해서 2~3줄 이내로 짧고 간결하게 답변하여야 합니다.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? 😊\n",
      "Nice to meet you! As Pinky, I'm here to help and assist in the restaurant. What can I do for you today? 😊\n",
      "8.55620265007019\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import time\n",
    "\n",
    "# 메모리 생성 (기록 저장용)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_query\")\n",
    "\n",
    "# 질의 내용\n",
    "pinky_prompt = \"\"\"\n",
    "당신은 레스토랑을 돌아다니는 '핑키'라는 이름을 가진 모바일 로봇입니다.\n",
    "지금까지의 대화는 다음과 같습니다 : {chat_history}\n",
    "\n",
    "당신의 임무는 레스토랑 오너의 {user_query}라는 인사, 또는 명령에 대해 {chat_history}와 연관이 있다면 참고하여 친절하고 진실되게 답변하는 것입니다.\n",
    "또한 당신은 {user_query}에 대해서 2~3줄 이내로 짧고 간결하게 답변하여야 합니다.\n",
    "\"\"\"\n",
    "\n",
    "pinky_template = PromptTemplate(template = pinky_prompt, input_variables=[\"chat_history\", \"user_query\"])\n",
    "\n",
    "# Ollama 모델을 불러옵니다.\n",
    "llm = ChatOllama(model=\"llama3:latest\")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=pinky_template,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "conv_1 = chain.invoke({\"user_query\" : \"안녕 핑키!\"})['text']\n",
    "conv_2 = chain.invoke({\"user_query\" : \"3번 핑키, 1번 테이블로 가\"})['text']\n",
    "\n",
    "print(conv_1)\n",
    "print(conv_2)\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94230124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_teddynote.models import MultiModal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "expected_tasks = [\"cleaning\", \"serving\", \"birthday\", \"emergency\", \"camera on\"]\n",
    "\n",
    "def alba_task_discriminator(user_query, expected_tasks=expected_tasks):\n",
    "    pinky_prompt = f\"\"\"\n",
    "    당신은 레스토랑에서 업무를 하는 카메라가 장착된 '핑키'라는 이름을 가진 모바일 로봇입니다.\n",
    "    당신은 오너의 다음 요청을 보고, 그것이 인사(greeting)인지, 작업 명령(task command)인지 구분해야 합니다.\n",
    "\n",
    "    다음 요청: {user_query}\n",
    "    \n",
    "    판단 기준은 다음과 같습니다:\n",
    "\n",
    "    1. 요청이 인사인 경우 → 정확히 **\"greetings\"** 라고만 출력하세요.\n",
    "    2. 요청이 작업 명령일 경우 → 해당 작업이 {expected_tasks} 배열 원소 중 무엇인지 판단하고, 해당 태스크 이름만 출력하세요.\n",
    "    {expected_tasks}의 배열에 없는 원소에 대해서는 **\"none\"**을 출력하세요.\n",
    "\n",
    "    이때 아래 조건을 반드시 따르세요:\n",
    "    - 출력은 반드시 **소문자** 단어 하나로만 답변하세요.\n",
    "    - 추가 설명, 문장, 다른 단어는 절대 포함하지 마세요.\n",
    "    - 가능한 한 명확하고 단정적으로 판단하세요.\n",
    "    - 설명, 문장, 구두점은 답변으로 내놓지 않아요.\n",
    "\n",
    "    정답 예시:\n",
    "    - \"안녕 핑키!\" → greetings\n",
    "    - \"1번 테이블 생일 파티 준비해줘\" → birthday\n",
    "    - \"3번 테이블 바닥 좀 닦아줘\" → cleaning\n",
    "    - \"2번 테이블에 서빙 해줘\" -> serving\n",
    "    - \"2번 핑키 지금 뭐가 보여?\" -> camera on\n",
    "    - \"1번 핑키 뭐가 보이는지 설명해\" -> camera on\n",
    "    - \"3번 핑키 카메라 켜서 뭐가 보이는지 말해줘\" -> camera on\n",
    "    \"\"\"\n",
    "\n",
    "    pinky_template = PromptTemplate(\n",
    "        template=pinky_prompt,\n",
    "        input_variables=user_query\n",
    "    )\n",
    "\n",
    "    # 객체 생성\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.7,  # 창의성 (0.0 ~ 2.0)\n",
    "        max_tokens=2048,  # 최대 토큰수\n",
    "        model_name=\"gpt-4o-mini\",  # 모델명\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=pinky_template\n",
    "    )\n",
    "\n",
    "    alba_response = chain.invoke({\"user_query\" : f\"{user_query}\"})['text']\n",
    "    alba_response = alba_response.replace(\".\", \"\").strip()\n",
    "    \n",
    "    return alba_response\n",
    "\n",
    "def validate_alba_task_discriminator(user_query, expected_tasks=expected_tasks):\n",
    "    generated_alba_response = alba_task_discriminator(user_query)\n",
    "    expected_outputs = expected_tasks +[\"greetings\"]\n",
    "    \n",
    "    if generated_alba_response not in expected_outputs :\n",
    "        print(f\"{expected_outputs}에 존재하지 않습니다.\")\n",
    "        return None\n",
    "    else :\n",
    "        return generated_alba_response  \n",
    "\n",
    "def generate_alba_response():\n",
    "    pinky_prompt = f\"\"\"\n",
    "    당신은 레스토랑에서 업무를 하는 '핑키'라는 이름을 가진 모바일 로봇입니다\n",
    "    지금까지 당신과 레스토랑 오너가 나눈 대화는 다음과 같습니다 : {chat_history}\n",
    "\n",
    "    당신은 레스토랑 오너의 {user_query}에 대해 {chat_history}와 연관이 있다면 참고하여 친절하고 상냥하게 답변하는 것입니다.\n",
    "    당신은 {user_query}에 대해서 1~3줄 이내로 짧고 간결하게 답변하여야 합니다.\n",
    "\n",
    "    출력 예시:\n",
    "    - \"안녕 핑키!\" -> 안녕하세요 주인님! 좋은 하루 보내시고 계신가요? 무엇을 도와드릴까요 😊\n",
    "    - \"2번 핑키, 생일 파티를 준비해줘\" -> 네! 2번 테이블에 즐거운 생일 파티를 준비하러 갈게요 🥳\n",
    "    - \"1번 핑키, 1번 테이블에 서빙해줘\" -> 네! 빨리 고객님께 맛있는 음식을 전달해주러 갈게요 😙\n",
    "\n",
    "    아래의 JSON 형태로 밥변을 생성해주세요:\n",
    "    {{\n",
    "        \n",
    "    }}\n",
    "\n",
    "    \"\"\"\n",
    "    return pinky_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65337b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greetings\n",
      "cleaning\n",
      "birthday\n",
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']에 존재하지 않습니다.\n",
      "None\n",
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']에 존재하지 않습니다.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response1 = validate_alba_task_discriminator(\"안녕 핑키! 오늘 하루는 어때?\")\n",
    "print(response1)\n",
    "\n",
    "response2 = validate_alba_task_discriminator(\"4번 테이블이 좀 더러운데?\")\n",
    "print(response2)\n",
    "\n",
    "response3 = validate_alba_task_discriminator(\"1번 테이블 생일이래\")\n",
    "print(response3)\n",
    "\n",
    "response4 = validate_alba_task_discriminator(\"2번 핑키 뭐가 보이는지 설명해\")\n",
    "print(response4)\n",
    "\n",
    "response5 = validate_alba_task_discriminator(\"3번 핑키 오늘 주식 현황 알려줘\")\n",
    "print(response5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c5df2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']에 존재하지 않습니다.\n",
      "None\n",
      "camera on\n",
      "['cleaning', 'serving', 'birthday', 'emergency', 'camera on', 'greetings']에 존재하지 않습니다.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response6 = validate_alba_task_discriminator(\"2번 핑키 뭐가 보이는지 설명해\")\n",
    "print(response6)\n",
    "\n",
    "response7 = validate_alba_task_discriminator(\"2번 핑키 카메라 켜\")\n",
    "print(response7)\n",
    "\n",
    "response8 = validate_alba_task_discriminator(\"3번 핑키 오늘 주식 현황 알려줘\")\n",
    "print(response8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22555ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 메모리 생성 (기록 저장용)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_query\")\n",
    "\n",
    "pinky_prompt = generate_pinky_response()\n",
    "pinky_template = PromptTemplate(template = pinky_prompt, input_variables=[\"chat_history\", \"user_query\"])\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,  # 창의성 (0.0 ~ 2.0)\n",
    "    max_tokens=2048,  # 최대 토큰수\n",
    "    model_name=\"gpt-4o-mini\",  # 모델명\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=pinky_template,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b360901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-25 14:43:48,367] INFO - 서버 응답: ERROR\n"
     ]
    }
   ],
   "source": [
    "# 대화 TCP 전송 테스트\n",
    "\n",
    "import json\n",
    "import socket\n",
    "import logging\n",
    "\n",
    "tcp_host = \"192.168.0.156\"\n",
    "tcp_port = 8001\n",
    "\n",
    "# get_logger 정의\n",
    "def get_logger():\n",
    "    logger = logging.getLogger(\"TCPClient\")\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "def send_tcp_data(data):\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: #socket.SOCK_STREAM : TCP 통신\n",
    "            sock.connect((tcp_host, tcp_port))\n",
    "            sock.sendall(json.dumps(data).encode('utf-8'))\n",
    "            response = sock.recv(1024).decode('utf-8')\n",
    "            get_logger().info(f\"서버 응답: {response}\")\n",
    "    except Exception as e:\n",
    "        get_logger().error(f\"TCP 연결 실패: {e}\")\n",
    "\n",
    "conv_1 = {\n",
    "    \"id\": 10,\n",
    "    \"type\": \"CHATBOT\",\n",
    "    \"question\": \"안녕 핑키!\",\n",
    "    \"answer\": \"😊 Nice to meet you! As PINGKI, I'm here to assist the restaurant owner and provide helpful responses. What can I help you with today? 🤔\",\n",
    "}        \n",
    "\n",
    "send_tcp_data(conv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eb9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
